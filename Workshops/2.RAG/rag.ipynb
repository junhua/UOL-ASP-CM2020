{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) ##\n",
    "\n",
    "Now we use LangChain to implement the same chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU \\\n",
    "#     langchain==0.0.292 \\\n",
    "#     openai==0.28.0 \\\n",
    "#     datasets==2.10.1 \\\n",
    "#     pinecone-client==2.2.4 \\\n",
    "#     tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model='gpt-3.5-turbo'\n",
    "    # model='gpt-4'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Writing a project proposal can be a complex task, but by breaking it down into smaller steps, you can approach it more effectively. Here's a step-by-step guide to help you through the process:\n",
      "\n",
      "1. Understand the Requirements: Begin by thoroughly understanding the project requirements outlined in the summary. Pay attention to the specific components that need to be included in your proposal.\n",
      "\n",
      "2. Define the Problem Statement: Clearly state the problem or need that your software project aims to address. This sets the context for your proposal and helps the reader understand the purpose and importance of the project.\n",
      "\n",
      "3. Conduct Research: Gather relevant information about the chosen domain and similar tools or projects. Identify the challenges and existing capabilities in the field. This research will help you make informed decisions and demonstrate your understanding of the project's context.\n",
      "\n",
      "4. Define the Scope: Clearly define the scope of your project by specifying what will and will not be delivered. This helps manage expectations and ensures that your proposal remains focused.\n",
      "\n",
      "5. Identify Stakeholders: Identify the key stakeholders involved in the project. Discuss how you plan to involve them in the requirements elicitation process, whether through literature sources or empirical proof such as usability studies.\n",
      "\n",
      "6. Design and Development Process: Describe the software development process you intend to follow, including any methodologies or frameworks you plan to use. Justify your choices and explain how they align with the project's goals.\n",
      "\n",
      "7. Deliverable Components: Identify the specific components or features of the software that you plan to develop. Justify why these components are necessary and how they contribute to solving the problem identified in the problem statement.\n",
      "\n",
      "8. Timescale and Milestones: Define a clear timescale for your project, including milestones and dependencies. Break down the work into activities and provide a timeline for each. Consider potential contingencies and how you plan to handle them.\n",
      "\n",
      "9. Formal Specification and User-Acceptance Criteria: Provide a formal specification of the desired system, including its functionality, user interface, and any technical requirements. Define user-acceptance criteria that will be used to test the software's effectiveness.\n",
      "\n",
      "10. Prototyping and Iterative Design: Explain how you will approach iterative design and development activities. Describe the early prototypes you plan to develop and discuss their strengths and weaknesses. Include any assumptions you will test and how you will validate your designs.\n",
      "\n",
      "11. Feasibility Evaluation: Critically evaluate the concept, current state of the project, and proposed software project. Provide a systematic assessment of its feasibility, considering factors such as technical challenges, resource requirements, and potential risks.\n",
      "\n",
      "12. Writing and Formatting: Write your proposal in a clear, concise, and well-structured manner. Pay attention to grammar, spelling, and formatting guidelines. Compile the proposal into a single PDF document as specified in the requirements.\n",
      "\n",
      "Remember to review and revise your proposal before submission to ensure it meets all the requirements and effectively communicates your project idea. Good luck with your project proposal!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a university computer science professor that aims to help students pass their assessments\"),\n",
    "    HumanMessage(content=\"\"\"\n",
    "Can you guide me through the process of writing a project proposal? A summary of the project requirement is as follows:\n",
    "\n",
    "The project proposal requirements for this coursework assignment involve designing and developing a software project. The proposal should be no longer than 8,000 words and should be submitted as a single PDF document. The proposal should explore design decisions, consider the context of use, and identify the software development process. It should include a justification for the deliverable components of the software and the necessary research and iterative prototyping to complete them. The proposal should also define the timescale of work, including dependencies, milestones, and contingencies, with a clear breakdown of work and activities. A formal specification of the desired system should be included, along with user-acceptance criteria for testing. The scope of the project should be clearly defined, indicating what will and will not be delivered. Evidence of requirements elicitation involving project stakeholders should be provided, either through literature sources or empirical proof such as usability studies. A research summary should highlight the challenges of the chosen domain and the capabilities of similar tools. The approach to the project should be described, including motivations and reasoning, and the tasks required to complete the project should be identified. Early prototypes should demonstrate iterative design and development activities, highlighting strengths and weaknesses. Assumption testing and validation of designs should also be included. Finally, a critical evaluation of the concept, current state of the project, and proposed software project should be provided, with a clear and systematic rhetoric and overall evaluation of feasibility.\"\"\"\n",
    "                 ),\n",
    "    # AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    # HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]\n",
    "res = chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a template for a software project proposal based on the requirements you provided:\n",
      "\n",
      "[Your Name]\n",
      "[Your Institution]\n",
      "[Date]\n",
      "\n",
      "[Title of the Proposal]\n",
      "\n",
      "1. Introduction\n",
      "   - Problem Statement: [Clearly state the problem or need that the software project aims to address]\n",
      "   - Objectives: [List the objectives and goals of the project]\n",
      "\n",
      "2. Research Summary\n",
      "   - Domain Challenges: [Discuss the challenges and complexities of the chosen domain]\n",
      "   - Existing Capabilities: [Highlight the capabilities of similar tools or projects]\n",
      "\n",
      "3. Scope\n",
      "   - Inclusions: [Clearly define what will be delivered as part of the software project]\n",
      "   - Exclusions: [Specify what will not be included in the project scope]\n",
      "\n",
      "4. Stakeholder Involvement\n",
      "   - Requirements Elicitation: [Explain how you plan to involve project stakeholders, either through literature sources or empirical proof such as usability studies]\n",
      "\n",
      "5. Design and Development Process\n",
      "   - Software Development Methodology: [Describe the development process or methodology you plan to follow]\n",
      "   - Justification: [Explain why the chosen methodology aligns with the project's goals]\n",
      "\n",
      "6. Deliverable Components\n",
      "   - Component 1: [Describe the first deliverable component and its justification]\n",
      "   - Component 2: [Describe the second deliverable component and its justification]\n",
      "   - ...\n",
      "\n",
      "7. Timescale and Milestones\n",
      "   - Timeline: [Provide a clear breakdown of work and activities with specific timeframes]\n",
      "   - Dependencies: [Identify any dependencies between tasks or milestones]\n",
      "   - Contingencies: [Discuss potential contingencies and your plan to handle them]\n",
      "\n",
      "8. Formal Specification and User-Acceptance Criteria\n",
      "   - System Specification: [Provide a formal specification of the desired system, including functionality and technical requirements]\n",
      "   - User-Acceptance Criteria: [Define criteria that will be used to test the software's effectiveness]\n",
      "\n",
      "9. Prototyping and Iterative Design\n",
      "   - Prototypes: [Describe the early prototypes you plan to develop and their strengths and weaknesses]\n",
      "   - Assumptions and Validation: [Discuss assumptions to be tested and the methods for validating designs]\n",
      "\n",
      "10. Feasibility Evaluation\n",
      "    - Concept Evaluation: [Critically evaluate the concept, current state of the project, and proposed software project]\n",
      "    - Feasibility Assessment: [Provide a systematic assessment of the project's feasibility, considering technical challenges, resource requirements, and potential risks]\n",
      "\n",
      "11. Conclusion\n",
      "    - Summary: [Summarize the key points discussed in the proposal]\n",
      "    - Recommendations: [Provide any recommendations or suggestions for the project]\n",
      "\n",
      "Please note that this is a general template, and you should adapt it to fit your specific project and requirements.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Can you generate a proposal template?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG ##\n",
    "\n",
    "### Giving a summary as context ###\n",
    "Recall that LLM has hallucination problem? Now let's use RAG to tackle it. The trivial approach is to provide a summary of LLama 2 as context and add it into the prompt. Let's see if the LLM can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 refers to the second version of the Llama programming language. Llama is a functional programming language that is designed to be simple, yet expressive. It focuses on immutability, referential transparency, and composability.\n",
      "\n",
      "Some of the special features of Llama 2 include:\n",
      "\n",
      "1. Purely Functional: Llama 2 is a purely functional programming language, meaning that it treats computation as the evaluation of mathematical functions. This allows for easier reasoning about code and facilitates the use of techniques like lazy evaluation and function composition.\n",
      "\n",
      "2. Immutability: Llama 2 promotes immutability, meaning that once a value is assigned, it cannot be changed. This helps in writing more robust and thread-safe code, as there are no concerns about unexpected side effects or concurrent modifications.\n",
      "\n",
      "3. Type Inference: Llama 2 incorporates type inference, which means that the language can automatically deduce the types of expressions and variables. This reduces the need for explicit type annotations and makes the code more concise and readable.\n",
      "\n",
      "4. Higher-Order Functions: Llama 2 supports higher-order functions, allowing functions to be passed as arguments, returned as results, and stored in data structures. This enables powerful functional programming techniques like map, filter, and reduce, as well as the creation of abstractions and reusable code.\n",
      "\n",
      "5. Pattern Matching: Llama 2 includes pattern matching, which allows developers to match the structure of data against predefined patterns. This feature simplifies complex conditional logic and provides a concise and elegant way to handle different cases or variants of data.\n",
      "\n",
      "Overall, Llama 2 aims to provide a language that is both simple and expressive, allowing developers to write clean and efficient code while leveraging the benefits of functional programming paradigms.\n"
     ]
    }
   ],
   "source": [
    "# now create a new user prompt\n",
    "messages = [HumanMessage(\n",
    "    content=\"What is so special about Llama 2?\"\n",
    ")]\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below delimited by tag <knowledge></knowledge>, answer the query delimited by tag <query></query>.\n",
      "\n",
      "Contexts:\n",
      "\n",
      "<knowledge>\n",
      "What is LLAMA 2?\n",
      "LLAMA 2 is an open-source language model that can be downloaded and run locally, with different versions available for download, including chat models and normal models.\n",
      "How does LLAMA 2 perform compared to other models?\n",
      "LLAMA 2 performs well in terms of sentiment scores and human evaluation results, outperforming open source chat models on most benchmarks.\n",
      "What are the trade-offs between safety and helpfulness in LLAMA 2?\n",
      "LLAMA 2 prioritizes both safety and helpfulness in its outputs, thanks to safety patterns and improvements such as a larger pre-training data set, grouped query attention, and reinforcement learning with human feedback.\n",
      "What are the safety improvements in LLAMA 2?\n",
      "LLAMA 2 incorporates safety improvements such as a larger pre-training data set, grouped query attention, and reinforcement learning with human feedback.\n",
      "Can LLAMA 2 be used for startups?\n",
      "Yes, the 13B model of LLAMA 2 is recommended for startups as it performs well on academic benchmarks.\n",
      "</knowledge>\n",
      "\n",
      "Query: \n",
      "\n",
      "<query>What's so special about LLama 2?</query>\n"
     ]
    }
   ],
   "source": [
    "llmchain_information = [\"\"\"\n",
    "What is LLAMA 2?\n",
    "LLAMA 2 is an open-source language model that can be downloaded and run locally, with different versions available for download, including chat models and normal models.\n",
    "How does LLAMA 2 perform compared to other models?\n",
    "LLAMA 2 performs well in terms of sentiment scores and human evaluation results, outperforming open source chat models on most benchmarks.\n",
    "What are the trade-offs between safety and helpfulness in LLAMA 2?\n",
    "LLAMA 2 prioritizes both safety and helpfulness in its outputs, thanks to safety patterns and improvements such as a larger pre-training data set, grouped query attention, and reinforcement learning with human feedback.\n",
    "What are the safety improvements in LLAMA 2?\n",
    "LLAMA 2 incorporates safety improvements such as a larger pre-training data set, grouped query attention, and reinforcement learning with human feedback.\n",
    "Can LLAMA 2 be used for startups?\n",
    "Yes, the 13B model of LLAMA 2 is recommended for startups as it performs well on academic benchmarks.\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)\n",
    "\n",
    "query = \"What's so special about LLama 2?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below delimited by tag <knowledge></knowledge>, answer the query delimited by tag <query></query>.\n",
    "\n",
    "Contexts:\n",
    "\n",
    "<knowledge>{source_knowledge}</knowledge>\n",
    "\n",
    "Query: \n",
    "\n",
    "<query>{query}</query>\"\"\"\n",
    "\n",
    "print (augmented_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA 2 is special because it is an open-source language model that can be downloaded and run locally. It offers different versions, including chat models and normal models, providing flexibility and versatility for users. Additionally, LLAMA 2 performs well compared to other models, with high sentiment scores and positive human evaluation results, particularly outperforming open source chat models on most benchmarks. It also prioritizes both safety and helpfulness in its outputs, incorporating safety patterns and improvements such as a larger pre-training data set, grouped query attention, and reinforcement learning with human feedback. This ensures that LLAMA 2 provides reliable and valuable responses while maintaining user safety. Furthermore, LLAMA 2 is recommended for startups, especially the 13B model, as it performs well on academic benchmarks. Overall, LLAMA 2 stands out for its performance, safety features, and suitability for various applications, making it a special language model.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a knowledge base ###\n",
    "Giving a brief summary as context could only make the LLM response relevant, but not enough to create meaningful answers. We will need to give LLM a large knowledge base to refer to.\n",
    "\n",
    "Next, we will use a collection of research publications that are related to LLama 2 as the knowledge base. \n",
    "\n",
    "We will use this dataset from Hugging Face\n",
    "\n",
    "https://huggingface.co/datasets/jamescalam/llama-2-arxiv-papers-chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LJHOLD/.conda/envs/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 409/409 [00:00<00:00, 785kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/jamescalam--llama-2-arxiv-papers-chunked to /Users/LJHOLD/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 14.4M/14.4M [00:00<00:00, 16.2MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 492.40it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/LJHOLD/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 4838\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to turn all the chunks into vectors (formally known as **embeddings**) and store it in a vector database.\n",
    "\n",
    "We will use **Pinecone** as the vector database.\n",
    "\n",
    "Go to https://app.pinecone.io and register a free account. Then go to **API Keys** to retrieve your api_key and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LJHOLD/.conda/envs/rag/lib/python3.9/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "str expected, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpinecone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# get API key from app.pinecone.io and environment from console\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m os\u001b[39m.\u001b[39;49menviron[\u001b[39m\"\u001b[39;49m\u001b[39mPINECONE_API_KEY\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mPINECONE_API_KEY\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pinecone\u001b[39m.\u001b[39minit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     api_key\u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mPINECONE_API_KEY\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     environment\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mus-east1-gcp\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/LJHOLD/Workspace/SIMGE/UOL-ASP-CM2020/Workshops/2.RAG/rag.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/rag/lib/python3.9/os.py:684\u001b[0m, in \u001b[0;36m_Environ.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setitem__\u001b[39m(\u001b[39mself\u001b[39m, key, value):\n\u001b[1;32m    683\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)\n\u001b[0;32m--> 684\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencodevalue(value)\n\u001b[1;32m    685\u001b[0m     putenv(key, value)\n\u001b[1;32m    686\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[key] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/.conda/envs/rag/lib/python3.9/os.py:756\u001b[0m, in \u001b[0;36m_createenviron.<locals>.encode\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(value):\n\u001b[1;32m    755\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 756\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstr expected, not \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    757\u001b[0m     \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39mencode(encoding, \u001b[39m'\u001b[39m\u001b[39msurrogateescape\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: str expected, not NoneType"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "# get API key from app.pinecone.io and environment from console\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") \n",
    "pinecone.init(\n",
    "    api_key= os.environ[\"PINECONE_API_KEY\"],\n",
    "    environment= \"us-east1-gcp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the dimension to 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the embeddings and add to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# for i in tqdm(range(0, len(data), batch_size)):\n",
    "#     i_end = min(len(data), i+batch_size)\n",
    "#     # get batch of data\n",
    "#     batch = data.iloc[i:i_end]\n",
    "#     # generate unique ids for each chunk\n",
    "#     ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "#     # get text to embed\n",
    "#     texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "#     # embed text\n",
    "#     embeds = embed_model.embed_documents(texts)\n",
    "#     # get metadata to store in Pinecone\n",
    "#     metadata = [\n",
    "#         {'text': x['chunk'],\n",
    "#          'source': x['source'],\n",
    "#          'title': x['title']} for i, x in batch.iterrows()\n",
    "#     ]\n",
    "#     # add to Pinecone\n",
    "#     index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation ###\n",
    "Finally, it's time to do some RAG! We'll be diving back into LangChain and reusing our template prompt from earlier.\n",
    "\n",
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a vectorstore. We pass in our vector index to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model, text_field\n",
    ")\n",
    "\n",
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have retrieved the top 3 most relevant text trunks. Next, we will attach the retrievd text to augment the prompt. Let's see if the results get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the contexts below delimited by tag <knowledge></knowledge>, answer the query delimited by tag <query></query>.\n",
      "\n",
      "Contexts:\n",
      "\n",
      "<knowledge>Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint</knowledge>\n",
      "\n",
      "Query: \n",
      "\n",
      "<query>What is so special about Llama 2?</query>\n"
     ]
    }
   ],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below delimited by tag <knowledge></knowledge>, answer the query delimited by tag <query></query>.\n",
    "\n",
    "Contexts:\n",
    "\n",
    "<knowledge>{source_knowledge}</knowledge>\n",
    "\n",
    "Query: \n",
    "\n",
    "<query>{query}</query>\"\"\"\n",
    "    return augmented_prompt\n",
    "\n",
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. These fine-tuned LLMs, specifically L/l.sc/a.sc/m.sc/a.sc/t.sc, are optimized for dialogue use cases. According to benchmarks and human evaluations for helpfulness and safety, Llama 2 models outperform open-source chat models and may be a suitable substitute for closed-source models. The approach to fine-tuning and safety in Llama 2 is described in detail, and it is highlighted that closed-source LLMs are heavily fine-tuned to align with human preferences, enhancing their usability and safety. Llama 2 aims to provide an open and efficient foundation for language models.\n"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to ask more questions and compare the answers with and without RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The safety measures used in the development of Llama 2 are not explicitly mentioned in the provided contexts.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the development of Llama 2, safety measures were taken to increase the safety of the models. These measures include safety-specific data annotation and tuning, red-teaming, and iterative evaluations. The goal of these safety measures is to improve the safety of the models and enable more responsible development of large language models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
